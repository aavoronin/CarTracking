Here are the expected value ranges for YOLO training metrics when the model is trained correctly (on a reasonably clean, annotated dataset):

âœ… Detection Metrics (on Validation Set)
Metric	Good Range	Excellent	Notes
metrics/precision(B)	0.70 â€“ 0.90+	> 0.90	High = few false positives
metrics/recall(B)	0.60 â€“ 0.85+	> 0.85	High = few missed detections
metrics/mAP50(B)	0.70 â€“ 0.95+	> 0.90	Easier mAP (IoU â‰¥ 0.50)
metrics/mAP50-95(B)	0.40 â€“ 0.75+	> 0.60	Stricter COCO-style mAP (IoU 0.5â€“0.95); lower is expected

ðŸ”¸ If mAP50-95 is low while mAP50 is high â†’ your model finds objects but isnâ€™t localizing them well.

ðŸ“‰ Loss Values
Metric	Good Range (Final Epochs)	Notes
train/box_loss	< 0.05 â€“ 0.20	Lower = better localization
train/cls_loss	< 0.01 â€“ 0.10	Lower = better classification
train/dfl_loss	< 0.01 â€“ 0.10	YOLOv8: measures box regression quality (distribution focal)
val/box_loss	Similar to train/box_loss	Should decrease and stabilize
val/cls_loss	Similar to train/cls_loss
val/dfl_loss	Similar to train/dfl_loss

ðŸ”¹ Loss should decrease and stabilize. A slightly higher val loss is normal due to generalization.

âš™ï¸ Learning Rates (lr/pg0, pg1, pg2)
Metric	Typical Values	Notes
lr/pg0	~1e-3 to 1e-5	Backbone learning rate
lr/pg1	~1e-3 to 1e-5	Head layers learning rate
lr/pg2	~1e-3 to 1e-5	Usually same as others unless customized

These depend on learning rate scheduling. They should decay slowly over time.

âš ï¸ Red Flags
Symptom	Possible Issue
mAP50 stays < 0.5	Model not learning; check labels, overfitting, poor augmentation
Loss stays flat or increases	Wrong labels, poor anchors, bad LR
High precision but low recall	Model is over-conservative (misses objects)

ðŸŽ¯ Real-World Targets (well-trained models on clean data)
mAP50: 0.85â€“0.95+

mAP50-95: 0.55â€“0.75

Precision: > 0.85

Recall: > 0.80

Losses: all < 0.1â€“0.2 by the end